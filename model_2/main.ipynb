{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "main.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNVpNkiM7sfUk9hgOnlB6MY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Bruno-GSilva/price-prediction/blob/main/model_2/main.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EDflnFJwNNz_"
      },
      "source": [
        "# Model 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xyZJT2yrNiQ8"
      },
      "source": [
        "### Mount Drive to access data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sV4-EmidNK9Z",
        "outputId": "d014d7c9-9299-42fb-facc-ebe9090320a4"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 345,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R7vNgf7yXxUa"
      },
      "source": [
        "### Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pgCFEpoqUHjJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "700b41c0-9ad7-44ea-9315-e5dcdd43c01b"
      },
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.functional import F\n",
        "import torch.optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "device = 'cpu' #torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)"
      ],
      "execution_count": 346,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "smxeF74EXziJ"
      },
      "source": [
        "## Data Loader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G_U4SZvmXkCS"
      },
      "source": [
        "class PriceDataset(Dataset):\n",
        "\n",
        "  def __init__(self, transform=None):\n",
        "\n",
        "    #Read CSV file\n",
        "    df = pd.read_csv(\"/content/drive/MyDrive/data/PETR4.SA.csv\", parse_dates=[0], infer_datetime_format=True)\n",
        "    df_clean = df.dropna().reset_index()\n",
        "\n",
        "    #Add collumn indicating when there is a time skip > 1 day\n",
        "    time_skip = [ 0 if index == 0 else 1 if (row['Date'] - df_clean['Date'][index - 1]).days > 1 else 0 for index, row in df_clean.iterrows() ]\n",
        "    df_clean['Skip dates'] = time_skip\n",
        "\n",
        "    #Filter usefull collumns\n",
        "    df_filter = df_clean.filter([ 'Close'])#  'Open', 'High', 'Low', 'Close'])#, 'Skip dates'])\n",
        "    data = df_filter.values[5000:]\n",
        "\n",
        "    #Scale input\n",
        "    #scaler = MinMaxScaler()\n",
        "    #data = scaler.fit_transform(data)\n",
        "\n",
        "    #Create samples\n",
        "    x_samples = []\n",
        "    y_samples = []\n",
        "    time_window = 30\n",
        "    for i in range (time_window, len(data)-time_window):\n",
        "      scaler = MinMaxScaler()\n",
        "      x_data = np.array(data[i-time_window : i])\n",
        "      scaler.fit(x_data)\n",
        "      scaled_x = scaler.transform(x_data)\n",
        "      x_samples.append(scaled_x)\n",
        "      #y_samples.append([1. if data[i+30,3] > data[i, 3] else 0.])\n",
        "      #y_samples.append([1. if data[i+30,0] > data[i, 0] else 0.])\n",
        "      y_sample = data[i+30]\n",
        "      y_scaled = scaler.transform(np.reshape(y_sample, (-1,1)))\n",
        "      y_samples.append(y_scaled)\n",
        "\n",
        "    \n",
        "    x_samples, y_samples = np.array(x_samples), np.array(y_samples)\n",
        "\n",
        "    #Transform int tensors\n",
        "    self.x_data = torch.from_numpy(x_samples)\n",
        "    self.y_data = torch.from_numpy(y_samples)\n",
        "    self.n_data = y_samples.shape[0]\n",
        "\n",
        "    #Set transform\n",
        "    self.transform = transform\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    sample = self.x_data[index], self.y_data[index]\n",
        "    if self.transform:\n",
        "            sample = self.transform(sample)\n",
        "    return sample\n",
        "\n",
        "  def __len__(self):\n",
        "    return self.n_data"
      ],
      "execution_count": 347,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lN9ZM1L9NwlR"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WSJe-_RmNwlS"
      },
      "source": [
        "class LSTMPrice(nn.Module):\n",
        "\n",
        "  def __init__(self, num_classes, input_size, hidden_size, num_layers, seq_length):\n",
        "    super(LSTMPrice, self).__init__()\n",
        "    self.num_classes = num_classes\n",
        "    self.input_size = input_size\n",
        "    self.hidden_size = hidden_size\n",
        "    self.num_layers = num_layers\n",
        "    self.seq_length = seq_length\n",
        "\n",
        "    self.lstm = nn.LSTM(input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, batch_first=True)\n",
        "    self.fc_1 = nn.Linear(hidden_size, 64)\n",
        "    self.fc_2 = nn.Linear(64, num_classes)\n",
        "\n",
        "  def forward(self, x):\n",
        "    h_0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size) #hidden state\n",
        "    c_0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size) #internal state\n",
        "    # Propagate input through LSTM\n",
        "    output, (hn, cn) = self.lstm(x, (h_0, c_0)) #lstm with input, hidden, and internal state\n",
        "    #out = torch.reshape(output, (output.size(0), -1))\n",
        "    hn = hn.view(-1, self.hidden_size) #reshaping the data for Dense layer next\n",
        "    out = F.relu(hn)\n",
        "    out = F.relu(self.fc_1(out)) #first Dense\n",
        "    out = self.fc_2(out) #Final Output\n",
        "\n",
        "    #Output with sigmoid\n",
        "    #out = torch.sigmoid(out)\n",
        "\n",
        "    return out      \n"
      ],
      "execution_count": 348,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eOxMIbpuRmxR"
      },
      "source": [
        "## Preparing for trainning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TvuAyNjUQo_p"
      },
      "source": [
        "dataset = PriceDataset()"
      ],
      "execution_count": 349,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AEXhf5gBQrSX"
      },
      "source": [
        "#Hyperparameters\n",
        "batch_size=4\n",
        "learning_rate=0.0001\n",
        "input_size=dataset[0][0].shape[1]\n",
        "seq_length = dataset[0][0].shape[0]\n",
        "hidden_size=2\n",
        "num_epochs=20\n",
        "num_layers=1\n",
        "num_classes=dataset[0][1].shape[0]"
      ],
      "execution_count": 350,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VxNhVKLuRg9B"
      },
      "source": [
        "train_loader = DataLoader(dataset=dataset,\n",
        "                          batch_size=batch_size,\n",
        "                          shuffle=True,\n",
        "                          num_workers=2)"
      ],
      "execution_count": 351,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ry_pRLhWRrU5",
        "outputId": "ec2bda38-e361-4d1a-dd7e-5c8135901df7"
      },
      "source": [
        "model = LSTMPrice(num_classes, input_size, hidden_size, num_layers, seq_length)\n",
        "\n",
        "criterion = nn.MSELoss()\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "example_input = dataset[0][0].view(1,30,1)\n",
        "#print(example_input)\n",
        "#model(example_input.float())\n",
        "dataset[0][1]"
      ],
      "execution_count": 358,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.6164]], dtype=torch.float64)"
            ]
          },
          "metadata": {},
          "execution_count": 358
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vPaHsELyX99i",
        "outputId": "a7d1fe23-34f1-4a97-d120-1383d1cab37a"
      },
      "source": [
        "n_total_steps = len(train_loader)\n",
        "for epoch in range(num_epochs):\n",
        "    for i, (inputs, labels) in enumerate(train_loader):  \n",
        "        if labels.shape[0] == batch_size:\n",
        "          inputs = inputs.float()\n",
        "          labels = labels.view(batch_size, 1).float()\n",
        "          \n",
        "          # Forward pass\n",
        "          outputs = model(inputs).float()\n",
        "          #print(outputs)\n",
        "          loss = criterion(outputs, labels)\n",
        "          \n",
        "          # Backward and optimize\n",
        "          optimizer.zero_grad()\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "          \n",
        "          if (i+1) % 10 == 0:\n",
        "              print (f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{n_total_steps}], Loss: {loss}')\n",
        "              #print(outputs)"
      ],
      "execution_count": 353,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/20], Step [10/94], Loss: 1.7129223346710205\n",
            "Epoch [1/20], Step [20/94], Loss: 8.076231002807617\n",
            "Epoch [1/20], Step [30/94], Loss: 20.30123519897461\n",
            "Epoch [1/20], Step [40/94], Loss: 9.732126235961914\n",
            "Epoch [1/20], Step [50/94], Loss: 2.0270543098449707\n",
            "Epoch [1/20], Step [60/94], Loss: 10.546252250671387\n",
            "Epoch [1/20], Step [70/94], Loss: 0.992142379283905\n",
            "Epoch [1/20], Step [80/94], Loss: 1.590135097503662\n",
            "Epoch [1/20], Step [90/94], Loss: 1.874682903289795\n",
            "Epoch [2/20], Step [10/94], Loss: 4.132444381713867\n",
            "Epoch [2/20], Step [20/94], Loss: 4.543850421905518\n",
            "Epoch [2/20], Step [30/94], Loss: 0.3235752582550049\n",
            "Epoch [2/20], Step [40/94], Loss: 1.7787123918533325\n",
            "Epoch [2/20], Step [50/94], Loss: 5.3101935386657715\n",
            "Epoch [2/20], Step [60/94], Loss: 2.7233948707580566\n",
            "Epoch [2/20], Step [70/94], Loss: 0.5187737941741943\n",
            "Epoch [2/20], Step [80/94], Loss: 6.040611267089844\n",
            "Epoch [2/20], Step [90/94], Loss: 0.7999733090400696\n",
            "Epoch [3/20], Step [10/94], Loss: 2.488240957260132\n",
            "Epoch [3/20], Step [20/94], Loss: 1.8824387788772583\n",
            "Epoch [3/20], Step [30/94], Loss: 5.688945293426514\n",
            "Epoch [3/20], Step [40/94], Loss: 0.311200350522995\n",
            "Epoch [3/20], Step [50/94], Loss: 1.332621693611145\n",
            "Epoch [3/20], Step [60/94], Loss: 6.656554222106934\n",
            "Epoch [3/20], Step [70/94], Loss: 7.2078070640563965\n",
            "Epoch [3/20], Step [80/94], Loss: 4.029208183288574\n",
            "Epoch [3/20], Step [90/94], Loss: 0.5221469402313232\n",
            "Epoch [4/20], Step [10/94], Loss: 0.8519637584686279\n",
            "Epoch [4/20], Step [20/94], Loss: 5.66316556930542\n",
            "Epoch [4/20], Step [30/94], Loss: 1.7539089918136597\n",
            "Epoch [4/20], Step [40/94], Loss: 9.342644691467285\n",
            "Epoch [4/20], Step [50/94], Loss: 5.656242847442627\n",
            "Epoch [4/20], Step [60/94], Loss: 3.0877671241760254\n",
            "Epoch [4/20], Step [70/94], Loss: 1.0706692934036255\n",
            "Epoch [4/20], Step [80/94], Loss: 1.2135138511657715\n",
            "Epoch [4/20], Step [90/94], Loss: 2.5987229347229004\n",
            "Epoch [5/20], Step [10/94], Loss: 4.325418949127197\n",
            "Epoch [5/20], Step [20/94], Loss: 18.089014053344727\n",
            "Epoch [5/20], Step [30/94], Loss: 3.214714527130127\n",
            "Epoch [5/20], Step [40/94], Loss: 1.0809810161590576\n",
            "Epoch [5/20], Step [50/94], Loss: 0.37146830558776855\n",
            "Epoch [5/20], Step [60/94], Loss: 0.6757561564445496\n",
            "Epoch [5/20], Step [70/94], Loss: 0.5852987170219421\n",
            "Epoch [5/20], Step [80/94], Loss: 1.1303834915161133\n",
            "Epoch [5/20], Step [90/94], Loss: 0.5759652853012085\n",
            "Epoch [6/20], Step [10/94], Loss: 0.6303671598434448\n",
            "Epoch [6/20], Step [20/94], Loss: 6.01469087600708\n",
            "Epoch [6/20], Step [30/94], Loss: 9.100992202758789\n",
            "Epoch [6/20], Step [40/94], Loss: 1.7997572422027588\n",
            "Epoch [6/20], Step [50/94], Loss: 2.360243320465088\n",
            "Epoch [6/20], Step [60/94], Loss: 6.858778476715088\n",
            "Epoch [6/20], Step [70/94], Loss: 6.312347412109375\n",
            "Epoch [6/20], Step [80/94], Loss: 0.6168901920318604\n",
            "Epoch [6/20], Step [90/94], Loss: 1.7605445384979248\n",
            "Epoch [7/20], Step [10/94], Loss: 5.524584770202637\n",
            "Epoch [7/20], Step [20/94], Loss: 0.5907719135284424\n",
            "Epoch [7/20], Step [30/94], Loss: 4.243693828582764\n",
            "Epoch [7/20], Step [40/94], Loss: 10.015618324279785\n",
            "Epoch [7/20], Step [50/94], Loss: 0.9013088345527649\n",
            "Epoch [7/20], Step [60/94], Loss: 3.5704355239868164\n",
            "Epoch [7/20], Step [70/94], Loss: 6.255086898803711\n",
            "Epoch [7/20], Step [80/94], Loss: 1.431490421295166\n",
            "Epoch [7/20], Step [90/94], Loss: 1.0850028991699219\n",
            "Epoch [8/20], Step [10/94], Loss: 0.2711145281791687\n",
            "Epoch [8/20], Step [20/94], Loss: 4.070189952850342\n",
            "Epoch [8/20], Step [30/94], Loss: 0.185291588306427\n",
            "Epoch [8/20], Step [40/94], Loss: 1.0607590675354004\n",
            "Epoch [8/20], Step [50/94], Loss: 8.046555519104004\n",
            "Epoch [8/20], Step [60/94], Loss: 1.8053598403930664\n",
            "Epoch [8/20], Step [70/94], Loss: 9.96846866607666\n",
            "Epoch [8/20], Step [80/94], Loss: 0.03264231979846954\n",
            "Epoch [8/20], Step [90/94], Loss: 14.004043579101562\n",
            "Epoch [9/20], Step [10/94], Loss: 1.5948154926300049\n",
            "Epoch [9/20], Step [20/94], Loss: 3.6053378582000732\n",
            "Epoch [9/20], Step [30/94], Loss: 8.443004608154297\n",
            "Epoch [9/20], Step [40/94], Loss: 9.019556999206543\n",
            "Epoch [9/20], Step [50/94], Loss: 2.7812917232513428\n",
            "Epoch [9/20], Step [60/94], Loss: 0.4565275311470032\n",
            "Epoch [9/20], Step [70/94], Loss: 7.426692008972168\n",
            "Epoch [9/20], Step [80/94], Loss: 2.3627212047576904\n",
            "Epoch [9/20], Step [90/94], Loss: 12.654031753540039\n",
            "Epoch [10/20], Step [10/94], Loss: 3.723076581954956\n",
            "Epoch [10/20], Step [20/94], Loss: 18.83315658569336\n",
            "Epoch [10/20], Step [30/94], Loss: 1.9432357549667358\n",
            "Epoch [10/20], Step [40/94], Loss: 2.0050787925720215\n",
            "Epoch [10/20], Step [50/94], Loss: 7.613126754760742\n",
            "Epoch [10/20], Step [60/94], Loss: 6.982916831970215\n",
            "Epoch [10/20], Step [70/94], Loss: 0.9886677861213684\n",
            "Epoch [10/20], Step [80/94], Loss: 1.608903169631958\n",
            "Epoch [10/20], Step [90/94], Loss: 4.25137996673584\n",
            "Epoch [11/20], Step [10/94], Loss: 0.7191357612609863\n",
            "Epoch [11/20], Step [20/94], Loss: 25.11865997314453\n",
            "Epoch [11/20], Step [30/94], Loss: 1.051755666732788\n",
            "Epoch [11/20], Step [40/94], Loss: 0.34471672773361206\n",
            "Epoch [11/20], Step [50/94], Loss: 1.2801933288574219\n",
            "Epoch [11/20], Step [60/94], Loss: 0.45169955492019653\n",
            "Epoch [11/20], Step [70/94], Loss: 6.408834457397461\n",
            "Epoch [11/20], Step [80/94], Loss: 10.967231750488281\n",
            "Epoch [11/20], Step [90/94], Loss: 1.253434658050537\n",
            "Epoch [12/20], Step [10/94], Loss: 0.537794828414917\n",
            "Epoch [12/20], Step [20/94], Loss: 1.2103805541992188\n",
            "Epoch [12/20], Step [30/94], Loss: 0.38380512595176697\n",
            "Epoch [12/20], Step [40/94], Loss: 6.173991680145264\n",
            "Epoch [12/20], Step [50/94], Loss: 1.9199646711349487\n",
            "Epoch [12/20], Step [60/94], Loss: 0.37491509318351746\n",
            "Epoch [12/20], Step [70/94], Loss: 0.4125756025314331\n",
            "Epoch [12/20], Step [80/94], Loss: 7.2591071128845215\n",
            "Epoch [12/20], Step [90/94], Loss: 3.497835874557495\n",
            "Epoch [13/20], Step [10/94], Loss: 1.4027512073516846\n",
            "Epoch [13/20], Step [20/94], Loss: 0.6060470342636108\n",
            "Epoch [13/20], Step [30/94], Loss: 1.0938167572021484\n",
            "Epoch [13/20], Step [40/94], Loss: 0.5143271684646606\n",
            "Epoch [13/20], Step [50/94], Loss: 0.9462369680404663\n",
            "Epoch [13/20], Step [60/94], Loss: 0.5599284768104553\n",
            "Epoch [13/20], Step [70/94], Loss: 0.3525305986404419\n",
            "Epoch [13/20], Step [80/94], Loss: 7.564389228820801\n",
            "Epoch [13/20], Step [90/94], Loss: 26.65047836303711\n",
            "Epoch [14/20], Step [10/94], Loss: 0.7794184684753418\n",
            "Epoch [14/20], Step [20/94], Loss: 4.561784744262695\n",
            "Epoch [14/20], Step [30/94], Loss: 1.8923670053482056\n",
            "Epoch [14/20], Step [40/94], Loss: 14.615937232971191\n",
            "Epoch [14/20], Step [50/94], Loss: 3.6008176803588867\n",
            "Epoch [14/20], Step [60/94], Loss: 0.3575912117958069\n",
            "Epoch [14/20], Step [70/94], Loss: 0.5189098119735718\n",
            "Epoch [14/20], Step [80/94], Loss: 0.3173739016056061\n",
            "Epoch [14/20], Step [90/94], Loss: 0.2777125835418701\n",
            "Epoch [15/20], Step [10/94], Loss: 0.5275439023971558\n",
            "Epoch [15/20], Step [20/94], Loss: 2.154050350189209\n",
            "Epoch [15/20], Step [30/94], Loss: 0.5185410976409912\n",
            "Epoch [15/20], Step [40/94], Loss: 9.404865264892578\n",
            "Epoch [15/20], Step [50/94], Loss: 1.2107017040252686\n",
            "Epoch [15/20], Step [60/94], Loss: 0.8943000435829163\n",
            "Epoch [15/20], Step [70/94], Loss: 0.4250732660293579\n",
            "Epoch [15/20], Step [80/94], Loss: 0.4943309426307678\n",
            "Epoch [15/20], Step [90/94], Loss: 0.9670575857162476\n",
            "Epoch [16/20], Step [10/94], Loss: 0.5444048643112183\n",
            "Epoch [16/20], Step [20/94], Loss: 3.044921875\n",
            "Epoch [16/20], Step [30/94], Loss: 1.6087690591812134\n",
            "Epoch [16/20], Step [40/94], Loss: 0.7109095454216003\n",
            "Epoch [16/20], Step [50/94], Loss: 0.08001087605953217\n",
            "Epoch [16/20], Step [60/94], Loss: 2.213541030883789\n",
            "Epoch [16/20], Step [70/94], Loss: 5.575764179229736\n",
            "Epoch [16/20], Step [80/94], Loss: 1.0079646110534668\n",
            "Epoch [16/20], Step [90/94], Loss: 2.472653388977051\n",
            "Epoch [17/20], Step [10/94], Loss: 3.170132875442505\n",
            "Epoch [17/20], Step [20/94], Loss: 6.749597072601318\n",
            "Epoch [17/20], Step [30/94], Loss: 2.978409767150879\n",
            "Epoch [17/20], Step [40/94], Loss: 3.7062840461730957\n",
            "Epoch [17/20], Step [50/94], Loss: 0.8925000429153442\n",
            "Epoch [17/20], Step [60/94], Loss: 0.45460250973701477\n",
            "Epoch [17/20], Step [70/94], Loss: 4.3190765380859375\n",
            "Epoch [17/20], Step [80/94], Loss: 1.6574081182479858\n",
            "Epoch [17/20], Step [90/94], Loss: 2.117032289505005\n",
            "Epoch [18/20], Step [10/94], Loss: 12.702505111694336\n",
            "Epoch [18/20], Step [20/94], Loss: 3.4723432064056396\n",
            "Epoch [18/20], Step [30/94], Loss: 0.9753334522247314\n",
            "Epoch [18/20], Step [40/94], Loss: 0.35978710651397705\n",
            "Epoch [18/20], Step [50/94], Loss: 9.779949188232422\n",
            "Epoch [18/20], Step [60/94], Loss: 14.857009887695312\n",
            "Epoch [18/20], Step [70/94], Loss: 0.9509618282318115\n",
            "Epoch [18/20], Step [80/94], Loss: 0.8584606051445007\n",
            "Epoch [18/20], Step [90/94], Loss: 2.254772901535034\n",
            "Epoch [19/20], Step [10/94], Loss: 0.9909046292304993\n",
            "Epoch [19/20], Step [20/94], Loss: 8.82797908782959\n",
            "Epoch [19/20], Step [30/94], Loss: 0.7443625926971436\n",
            "Epoch [19/20], Step [40/94], Loss: 0.21060055494308472\n",
            "Epoch [19/20], Step [50/94], Loss: 1.8184298276901245\n",
            "Epoch [19/20], Step [60/94], Loss: 7.536153793334961\n",
            "Epoch [19/20], Step [70/94], Loss: 0.9709821939468384\n",
            "Epoch [19/20], Step [80/94], Loss: 18.73159408569336\n",
            "Epoch [19/20], Step [90/94], Loss: 1.9049862623214722\n",
            "Epoch [20/20], Step [10/94], Loss: 11.676279067993164\n",
            "Epoch [20/20], Step [20/94], Loss: 3.7094998359680176\n",
            "Epoch [20/20], Step [30/94], Loss: 1.7239437103271484\n",
            "Epoch [20/20], Step [40/94], Loss: 0.5092739462852478\n",
            "Epoch [20/20], Step [50/94], Loss: 4.837374687194824\n",
            "Epoch [20/20], Step [60/94], Loss: 18.241050720214844\n",
            "Epoch [20/20], Step [70/94], Loss: 0.10073584318161011\n",
            "Epoch [20/20], Step [80/94], Loss: 15.626458168029785\n",
            "Epoch [20/20], Step [90/94], Loss: 1.517479419708252\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gu7iQft-cm_9",
        "outputId": "f4b80c9e-d99f-49be-a4f5-9665bb3aaabd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(model(example_input.float()))"
      ],
      "execution_count": 359,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.0804]], grad_fn=<AddmmBackward>)\n"
          ]
        }
      ]
    }
  ]
}